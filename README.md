# Llama 3.0 From Scratch: Educational Implementation & Tiny Shakespeare Training

This repository contains an educational, from-scratch implementation of the Llama 3.0 language model. The goal is to deeply understand the inner workings of large language models (LLMs) and reinforce learning by building its fundamental components from the ground up.

**Important Note:** This project is **for educational purposes only**. It is **not** a fully-fledged, production-ready, optimized Llama 3.0 implementation. It has been trained on the Tiny Shakespeare dataset, and performance expectations should be aligned accordingly. The main focus is on code clarity, the learning process, and the implementation of core concepts.

## Project Features

This implementation includes the following key features:

* **Llama 3.0 Architecture Implemented From Scratch:** The Transformer architecture, attention mechanisms, and fundamental building blocks of Llama 3.0 are coded from the ground up.
* **RoPE (Rotary Positional Embeddings):** The RoPE mechanism is implemented for positional encoding.
* **KV-Cache (Key-Value Cache):** The KV-Cache mechanism is included for efficient inference.
* **Custom BPE (Byte Pair Encoding) Implementation:** A BPE algorithm was written from scratch for data preprocessing and tokenization. [Codes are taken from this repository, I just added explanations.](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb)
* **Training with Tiny Shakespeare Dataset:** The project is trained on the small and widely used Tiny Shakespeare dataset.
* **Readable and Well-Commented Code:**  Care has been taken to ensure the code is understandable and learning-oriented. Important sections are explained in detail with comments.
